import gradio as gr
from langchain.tools import Tool
import os 
import requests
import json
from datetime import datetime
import PyPDF2
import docx
from io import BytesIO
import ssl
import certifi
import nest_asyncio
from langchain_community.utilities import GoogleSearchAPIWrapper

# =============================================================================
# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
# =============================================================================

# æ¨¡å‹ç›¸é—œå‡½æ•¸
from core.config import (
    get_available_model,
    generate_response,
    check_model_config,
    chatgpt_system_prompt,
    OpenAI_Model,
    openai_client
)

# æœå°‹ç›¸é—œå‡½æ•¸
from core.search import (
    get_search,
    get_page_content,
    check_search_config
)

# æª”æ¡ˆè™•ç†ç›¸é—œå‡½æ•¸
from core.file_processing import (
    process_uploaded_file,
    extract_course_info
)

# å·¥å…·å‡½æ•¸
from core.chunk import (
    num_tokens_from_string,
    chunk_texts
)

# =============================================================================
# æª”æ¡ˆè™•ç†å’Œèª²ç¨‹è³‡è¨Šæå–
# =============================================================================


def process_course_content_with_chunking(course_info, user_requirements):
    """è™•ç†éé•·çš„èª²ç¨‹å…§å®¹ï¼Œé€²è¡Œåˆ†å¡Šè™•ç†"""
    
    # æª¢æŸ¥ç¸½ token æ•¸
    total_content = f"Course infoï¼š\n{course_info}\n\nUser requirementsï¼š\n{user_requirements}"
    total_tokens = num_tokens_from_string(total_content)
    
    print(f"[INFO] ç¸½è¼¸å…¥ tokens: {total_tokens}")
    
    # å¦‚æœè¶…éé™åˆ¶ï¼Œé€²è¡Œåˆ†å¡Šè™•ç†
    if total_tokens > 3000:  # ä¿ç•™ç©ºé–“çµ¦ prompt å’Œè¼¸å‡º
        print(f"[INFO] å…§å®¹éé•·ï¼Œé€²è¡Œåˆ†å¡Šè™•ç†...")
        
        # åˆ†å¡Šè™•ç†èª²ç¨‹è³‡è¨Š
        if num_tokens_from_string(course_info) > 2000:
            course_chunks = chunk_texts(course_info, 1500)
            
            # å°æ¯å€‹å¡Šæå–é—œéµè³‡è¨Š
            extracted_info = []
            for i, chunk in enumerate(course_chunks):
                print(f"[INFO] è™•ç†èª²ç¨‹è³‡è¨Šå¡Š {i+1}/{len(course_chunks)}")
                chunk_info = extract_course_info(chunk)
                extracted_info.append(chunk_info)
            
            # åˆä½µæå–çš„è³‡è¨Š
            course_info = combine_extracted_info(extracted_info)
    
    return course_info, user_requirements

def combine_extracted_info(extracted_info_list):
    """åˆä½µå¤šå€‹æå–çš„èª²ç¨‹è³‡è¨Š"""
    combine_prompt = '''
    Merge the following multiple course information fragments into a complete, structured course description:
    
    Please integrate and remove duplicate information, retaining the most important course characteristics.
    '''
    
    combined_content = "\n\n".join([f"Fragment {i+1}:\n{info}" for i, info in enumerate(extracted_info_list)])
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"{combined_content}\n\n{combine_prompt}"
        }
    ]
    
    try:
        return generate_response(messages, temperature=0.3, max_tokens=1500)
    except Exception as e:
        print(f"åˆä½µè³‡è¨Šå¤±æ•—: {str(e)}")
        return "\n".join(extracted_info_list)

def enhance_section_wrapper(q, course_info, section, search_content):
    """enhance_section_with_content çš„ wrapper å‡½æ•¸"""
    result = enhance_section_with_content(course_info, section, search_content)
    q.put(result)

def format_final_outline_wrapper(q, course_info, outline):
    """format_course_outline çš„ wrapper å‡½æ•¸"""
    result = format_course_outline(course_info, outline)
    q.put(result)

def generate_course_outline_with_rat(course_info, user_requirements, search_enabled=True):
    """ä½¿ç”¨å®Œæ•´ RAT æŠ€è¡“ç”Ÿæˆèª²ç¨‹å¤§ç¶±"""
    
    # æ­¥é©Ÿ1: è™•ç†éé•·å…§å®¹
    processed_course_info, processed_requirements = process_course_content_with_chunking(
        course_info, user_requirements
    )
    
    # æ­¥é©Ÿ2: ç”Ÿæˆåˆå§‹å¤§ç¶±
    print(f"{datetime.now()} [INFO] ç”Ÿæˆåˆå§‹èª²ç¨‹å¤§ç¶±...")
    initial_outline = get_course_draft(processed_course_info, processed_requirements)
    
    if not search_enabled:
        return initial_outline, initial_outline
    
    # æ­¥é©Ÿ3: RAT å¼åˆ†æ®µè™•ç†
    print(f"{datetime.now()} [INFO] å°èª²ç¨‹å¤§ç¶±é€²è¡Œ RAT åˆ†æ®µè™•ç†...")
    outline_sections = split_course_outline(initial_outline)
    print(f"{datetime.now()} [INFO] å¤§ç¶±åˆ†ç‚º {len(outline_sections)} å€‹æ®µè½")
    
    enhanced_outline = ""
    
    # æ­¥é©Ÿ4: é€æ®µæœå°‹å’Œå„ªåŒ– (RAT æ ¸å¿ƒæŠ€è¡“)
    for i, section in enumerate(outline_sections):
        print(f"{datetime.now()} [INFO] è™•ç†æ®µè½ {i+1}/{len(outline_sections)}...")
        enhanced_outline += "\n\n" + section
        
        # ç‚ºç•¶å‰æ®µè½ç”Ÿæˆå°ˆé–€çš„æœå°‹æŸ¥è©¢
        print(f"{datetime.now()} [INFO] ç‚ºæ®µè½ç”Ÿæˆæœå°‹æŸ¥è©¢...")
        section_query = generate_section_query(processed_course_info, section)
        print(f">>> æ®µè½ {i+1} æŸ¥è©¢: {section_query}")
        
        # æœå°‹ç›¸é—œè³‡æº
        print(f"{datetime.now()} [INFO] æœå°‹æ®µè½ç›¸é—œè³‡æº...")
        search_results = run_with_timeout(get_content_wrapper, 30, section_query)
        
        if search_results:
            # ä½¿ç”¨æœå°‹çµæœå„ªåŒ–ç•¶å‰æ®µè½
            LIMIT = 4
            for j, content in enumerate(search_results[:LIMIT]):
                print(f"{datetime.now()} [INFO] ä½¿ç”¨æœå°‹çµæœå„ªåŒ–æ®µè½ [{j+1}/{LIMIT}]...")
                
                optimized_section = run_with_timeout(
                    enhance_section_wrapper, 30,
                    processed_course_info, section, content
                )
                
                if optimized_section:
                    enhanced_outline = enhanced_outline.replace(section, optimized_section)
                    section = optimized_section  # æ›´æ–°æ®µè½ç”¨æ–¼ä¸‹æ¬¡å„ªåŒ–
                    print(f"{datetime.now()} [INFO] æ®µè½å„ªåŒ–å®Œæˆ [{j+1}/{LIMIT}]")
    
    # æ­¥é©Ÿ5: æœ€çµ‚æ•´åˆå’Œæ ¼å¼åŒ–
    print(f"{datetime.now()} [INFO] æœ€çµ‚æ•´åˆèª²ç¨‹å¤§ç¶±...")
    final_outline = run_with_timeout(
        format_final_outline_wrapper, 30,
        processed_course_info, enhanced_outline
    )
    
    if not final_outline:
        final_outline = enhanced_outline
    
    print(f"{datetime.now()} [INFO] èª²ç¨‹å¤§ç¶±ç”Ÿæˆå®Œæˆ")
    return initial_outline, final_outline

def split_course_outline(outline):
    """æ™ºèƒ½åˆ†å‰²èª²ç¨‹å¤§ç¶±ç‚ºé‚è¼¯æ®µè½"""
    split_prompt = '''
    Split the course outline into logical sections, where each section should contain a complete concept or topic.
    Use ## as a separator to divide sections.
    Keep the original content unchanged, just add separator symbols.
    '''
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"Course outlineï¼š\n{outline}\n\n{split_prompt}"
        }
    ]
    
    try:
        split_result = generate_response(messages, temperature=0.3, max_tokens=2000)
        sections = [section.strip() for section in split_result.split('##') if section.strip()]
        return sections
    except Exception as e:
        print(f"æ™ºèƒ½åˆ†å‰²å¤±æ•—ï¼Œä½¿ç”¨é è¨­åˆ†å‰²: {str(e)}")
        # é è¨­åˆ†å‰²æ–¹å¼
        return outline.split('\n\n')

def generate_section_query(course_info, section):
    """ç‚ºç‰¹å®šæ®µè½ç”Ÿæˆæœå°‹æŸ¥è©¢"""
    query_prompt = '''
    Based on the course information and current section content, generate a precise search query.
    The query should be able to find teaching resources, best practices, or case studies related to this section.
    
    Only output the search query, do not add other explanations.
    '''
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"Course informationï¼š\n{course_info}\n\nCurrent sectionï¼š\n{section}\n\n{query_prompt}"
        }
    ]
    
    try:
        return generate_response(messages, temperature=0.5, max_tokens=200)
    except Exception as e:
        return f"teaching methods {section[:50]}"

def enhance_section_with_content(course_info, section, search_content):
    """ä½¿ç”¨æœå°‹å…§å®¹å„ªåŒ–æ®µè½"""
    enhance_prompt = '''
    Based on the searched relevant teaching resources, improve and enrich the current course outline section.
    
    Improvement focus:
    1. Add specific teaching activities and methods
    2. Supplement relevant learning resources
    3. Include practical cases or exercises
    4. Optimize learning objective descriptions
    
    Maintain the core structure of the section while significantly improving content quality.
    '''
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"Course informationï¼š\n{course_info}\n\nCurrent sectionï¼š\n{section}\n\nSearch resourcesï¼š\n{search_content}\n\n{enhance_prompt}"
        }
    ]
    
    try:
        return generate_response(messages, temperature=0.6, max_tokens=1500)
    except Exception as e:
        print(f"æ®µè½å„ªåŒ–å¤±æ•—: {str(e)}")
        return section

def get_course_draft(course_info, user_requirements):
    """ç”ŸæˆåŸºç¤èª²ç¨‹å¤§ç¶±"""
    course_prompt = '''
    Based on the provided course information and user requirements, generate a detailed WEEKLY course outline for a 16-week semester. Please structure it as follows:

    **IMPORTANT WEEKLY FORMAT REQUIREMENTS:**
    - Organize content by weeks (Week 1, Week 2, etc.)
    - Each week should have specific learning topics and activities
    - Complex topics can span multiple weeks (e.g., arrays taught across Week 1 and Week 2)
    - Include both theoretical concepts and practical implementations
    - Provide specific subtopics and activities for each week
    - Consider that students need time to absorb and practice
    - Start with basic concepts and slowly build complexity

    **WEEKLY STRUCTURE TEMPLATE:**
    
    ## Week X: [Main Topic/Chapter]
    **Learning Objectives:**
    - [Specific learning goal 1]
    - [Specific learning goal 2]
    
    **Content Coverage:**
    - [Subtopic 1]: [Brief description]
    - [Subtopic 2]: [Brief description]
    - [Subtopic 3]: [Brief description]
    
    **Activities & Practice:**
    - [Lab exercise or coding practice]
    - [Assignment or project work]
    
    **Assessment:**
    - [Quiz, assignment, or evaluation method]

    **Example Format:**
    
    ## Week 1: Arrays - Fundamentals
    **Learning Objectives:**
    - Understand array concepts and memory allocation
    - Learn array declaration and initialization
    
    **Content Coverage:**
    - Array definition and characteristics
    - One-dimensional array syntax
    - Memory layout and indexing
    - Basic array operations (input/output)
    
    **Activities & Practice:**
    - Lab: Basic array exercises
    - Practice: Array input and display programs
    
    **Assessment:**
    - Programming Quiz 1: Array basics
    
    ## Week 2: Arrays - Advanced Operations
    **Learning Objectives:**
    - Master array manipulation techniques
    - Implement array algorithms
    
    **Content Coverage:**
    - Array searching algorithms (linear search)
    - Array sorting basics (bubble sort)
    - Multi-dimensional arrays introduction
    - Array functions and parameter passing
    
    **Activities & Practice:**
    - Lab: Implement search and sort algorithms
    - Assignment 1: Array processing program
    
    **Assessment:**
    - Programming Assignment 1 due

    Please generate a complete 16-week outline following this format, ensuring:
    1. **Logical Progression**: Topics build upon each other
    2. **Practical Focus**: Each week includes hands-on coding
    3. **Flexible Pacing**: Complex topics can span multiple weeks
    4. **Regular Assessment**: Include quizzes, assignments, and projects
    5. **Real Application**: Connect theory to practical programming
    '''
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"Course informationï¼š\n{course_info}\n\nUser requirementsï¼š\n{user_requirements}\n\n{course_prompt}"
        }
    ]
    
    try:
        return generate_response(messages, temperature=0.7, max_tokens=3500)  # å¢åŠ  token æ•¸
    except Exception as e:
        return f"ç”ŸæˆåŸºç¤å¤§ç¶±å¤±æ•—: {str(e)}"

def generate_course_search_query(course_info, user_requirements):
    """ç”Ÿæˆèª²ç¨‹ç›¸é—œçš„æœå°‹æŸ¥è©¢"""
    query_prompt = '''
    Based on the course information and user requirements, generate an effective search query to find relevant course design resources, teaching syllabi, or similar course information.
    The query should be concise but specific, able to find high-quality educational resources.

    Only output the search query, do not add other explanations.
    '''
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"Course informationï¼š\n{course_info}\n\nUser requirementsï¼š\n{user_requirements}\n\n{query_prompt}"
        }
    ]
    
    try:
        return generate_response(messages, temperature=0.5, max_tokens=200)
    except Exception as e:
        return f"course curriculum syllabus {course_info[:100]}"

def enhance_course_outline(course_info, base_outline, search_results):
    """ä½¿ç”¨æœå°‹çµæœå¢å¼·èª²ç¨‹å¤§ç¶±"""
    enhance_prompt = '''
    Based on the searched relevant educational resources and course information, improve and enhance the existing course outline. Please:

    1. **Content Enrichment**: Add excellent teaching concepts and methods from search results
    2. **Structure Optimization**: Reference structural designs from similar courses
    3. **Resource Supplementation**: Add relevant learning resources and reference materials
    4. **Practical Activities**: Integrate real cases and exercise activities
    5. **Assessment Improvement**: Improve evaluation methods and standards

    Maintain the core structure of the original outline while significantly enhancing content depth and breadth.
    Output in Markdown format.

    Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.
    '''
    
    # æ•´åˆæœå°‹çµæœ
    search_content = "\n\n".join([f"Reference material {i+1}:\n{content}" for i, content in enumerate(search_results)])
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"Original course informationï¼š\n{course_info}\n\nBase outlineï¼š\n{base_outline}\n\nSearched reference materialsï¼š\n{search_content}\n\n{enhance_prompt}"
        }
    ]
    
    try:
        return generate_response(messages, temperature=0.6, max_tokens=3000)
    except Exception as e:
        print(f"å¢å¼·å¤§ç¶±å¤±æ•—: {str(e)}")
        return base_outline

def format_course_outline(course_info, outline):
    """æœ€çµ‚æ ¼å¼åŒ–èª²ç¨‹å¤§ç¶±"""
    format_prompt = '''
    Perform final formatting and structural adjustment of the course outline, ensuring:

    1. **Clear Title Hierarchy**: Use appropriate Markdown headings
    2. **Logical Sequence**: Ensure content flows smoothly and progressively
    3. **Practical Details**: Include specific learning activities and time arrangements
    4. **Professional Presentation**: Suitable for teaching and learning use

    Output a complete, professional course outline.
    '''
    
    messages = [
        {
            "role": "system",
            "content": chatgpt_system_prompt
        },
        {
            "role": "user",
            "content": f"Course informationï¼š\n{course_info}\n\nCourse outlineï¼š\n{outline}\n\n{format_prompt}"
        }
    ]
    
    try:
        return generate_response(messages, temperature=0.4, max_tokens=3000)
    except Exception as e:
        print(f"æ ¼å¼åŒ–å¤±æ•—: {str(e)}")
        return outline

import tiktoken


def chunk_text_by_sentence(text, chunk_size=2048):
    """Chunk the $text into sentences with less than 2k tokens."""
    sentences = text.split('. ')
    chunked_text = []
    curr_chunk = []
    # é€å¥æ·»åŠ æ–‡æœ¬ç‰‡æ®µï¼Œç¡®ä¿æ¯ä¸ªæ®µè½éƒ½å°äº2kä¸ªtoken
    for sentence in sentences:
        if num_tokens_from_string(". ".join(curr_chunk)) + num_tokens_from_string(sentence) + 2 <= chunk_size:
            curr_chunk.append(sentence)
        else:
            chunked_text.append(". ".join(curr_chunk))
            curr_chunk = [sentence]
    # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
    if curr_chunk:
        chunked_text.append(". ".join(curr_chunk))
    return chunked_text[0]

def chunk_text_front(text, chunk_size = 2048):
    '''
    get the first `trunk_size` token of text
    '''
    chunked_text = ""
    tokens = num_tokens_from_string(text)
    if tokens < chunk_size:
        return text
    else:
        ratio = float(chunk_size) / tokens
        char_num = int(len(text) * ratio)
        return text[:char_num]


from datetime import datetime

from openai import OpenAI
import openai
import os

chatgpt_system_prompt = f'''
You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.
Knowledge cutoff: 2023-04
Current date: {datetime.now().strftime('%Y-%m-%d')}
'''

def get_draft(question):
    # Getting the draft answer
    draft_prompt = '''
IMPORTANT:
Try to answer this question/instruction with step-by-step thoughts and make the answer more structural.
Use `\n\n` to split the answer into several paragraphs.
Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.
'''
    draft = openai_client.chat.completions.create(
        model=OpenAI_Model,
        messages=[
            {
                "role": "system",
                "content": chatgpt_system_prompt
            },
            {
                "role": "user",
                "content": f"{question}" + draft_prompt
            }
        ],
        temperature = 1.0
    ).choices[0].message.content
    return draft

def split_draft(draft, split_char = '\n\n'):
    # å°†draftåˆ‡åˆ†ä¸ºå¤šä¸ªæ®µè½
    # split_char: '\n\n'
    paragraphs = draft.split(split_char)
    draft_paragraphs = [para for para in paragraphs if len(para)>5]
    # print(f"The draft answer has {len(draft_paragraphs)}")
    return draft_paragraphs

def split_draft_openai(question, answer, NUM_PARAGRAPHS = 4):
    split_prompt = f'''
Split the answer of the question into multiple paragraphs with each paragraph containing a complete thought.
The answer should be splited into less than {NUM_PARAGRAPHS} paragraphs.
Use ## as splitting char to seperate the paragraphs.
So you should output the answer with ## to split the paragraphs.
**IMPORTANT**
Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.
'''
    splited_answer = openai_client.chat.completions.create(
        model=OpenAI_Model,
        messages=[
            {
                "role": "system",
                "content": chatgpt_system_prompt
            },
            {
                "role": "user",
                "content": f"##Question: {question}\n\n##Response: {answer}\n\n##Instruction: {split_prompt}"
            }
        ],
        temperature = 1.0
    ).choices[0].message.content
    split_draft_paragraphs = split_draft(splited_answer, split_char = '##')
    return split_draft_paragraphs

def get_query(question, answer):
    query_prompt = '''
I want to verify the content correctness of the given question, especially the last sentences.
Please summarize the content with the corresponding question.
This summarization will be used as a query to search with Bing search engine.
The query should be short but need to be specific to promise Bing can find related knowledge or pages.
Try to make the query as relevant as possible to the last few sentences in the content.
**IMPORTANT**
Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.
'''
    query = openai_client.chat.completions.create(
        model=OpenAI_Model,
        messages=[
            {
                "role": "system",
                "content": chatgpt_system_prompt
            },
            {
                "role": "user",
                "content": f"##Question: {question}\n\n##Content: {answer}\n\n##Instruction: {query_prompt}"
            }
        ],
        temperature = 1.0
    ).choices[0].message.content
    return query

def get_content(query):
    """ç²å–ç¶²é å…§å®¹"""
    try:
        # æœå°‹çµæœ
        res = get_search(query, 3)
        if not res:
            print(">>> No Google Search Result found")
            return None
        
        search_results = res[0]
        link = search_results['link']
        print(f">>> Fetching content from: {link}")
        
        # ä½¿ç”¨å®‰å…¨çš„å…§å®¹æŠ“å–
        content = get_page_content(link)
        
        if not content:
            print(f">>> No content found for {link}")
            return None
        
        # åˆ†å¡Šè™•ç†
        chunked_texts = chunk_texts(content, 1500)
        cleaned_texts = [text.replace('\n', ' ').strip() for text in chunked_texts if text.strip()]
        
        return cleaned_texts[:3]  # é™åˆ¶è¿”å›æ•¸é‡
        
    except Exception as e:
        print(f">>> Content retrieval error: {str(e)}")
        return None


def get_revise_answer(question, answer, content):
    revise_prompt = '''
I want to revise the answer according to retrieved related text of the question in WIKI pages.
You need to check whether the answer is correct.
If you find some errors in the answer, revise the answer to make it better.
If you find some necessary details are ignored, add it to make the answer more plausible according to the related text.
If you find the answer is right and do not need to add more details, just output the original answer directly.
**IMPORTANT**
Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.
Add more details from retrieved text to the answer.
Split the paragraphs with \n\n characters.
Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.
'''

    revised_answer = openai_client.chat.completions.create(
        model=OpenAI_Model,
        messages=[
                {
                    "role": "system",
                    "content": chatgpt_system_prompt
                },
                {
                    "role": "user",
                    "content": f"##Existing Text in Wiki Web: {content}\n\n##Question: {question}\n\n##Answer: {answer}\n\n##Instruction: {revise_prompt}"
                }
            ],
            temperature = 1.0
    ).choices[0].message.content
    return revised_answer

def get_reflect_answer(question, answer):
    reflect_prompt = '''
Give a title for the answer of the question.
And add a subtitle to each paragraph in the answer and output the final answer using markdown format. 
This will make the answer to this question look more structured for better understanding.
**IMPORTANT**
Try to keep the structure (multiple paragraphs with its subtitles) in the response and make it more structual for understanding.
Split the paragraphs with \n\n characters.
Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.
'''
    reflected_answer = openai_client.chat.completions.create(
        model=OpenAI_Model,
        messages=[
                {
                    "role": "system",
                    "content": chatgpt_system_prompt
                },
                {
                    "role": "user",
                    "content": f"##Question:\n{question}\n\n##Answer:\n{answer}\n\n##Instruction:\n{reflect_prompt}"
                }
            ],
            temperature = 1.0
    ).choices[0].message.content
    return reflected_answer

def get_query_wrapper(q, question, answer):
    result = get_query(question, answer)
    q.put(result)  # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—

def get_content_wrapper(q, query):
    result = get_content(query)
    q.put(result)  # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—

# def get_revise_answer_wrapper(q, question, answer, content):
#     result = get_revise_answer(question, answer, content)
#     q.put(result)

# def get_reflect_answer_wrapper(q, question, answer):
#     result = get_reflect_answer(question, answer)
#     q.put(result)

from multiprocessing import Process, Queue
def run_with_timeout(func, timeout, *args, **kwargs):
    q = Queue()  # åˆ›å»ºä¸€ä¸ªQueueå¯¹è±¡ç”¨äºè¿›ç¨‹é—´é€šä¿¡
    # åˆ›å»ºä¸€ä¸ªè¿›ç¨‹æ¥æ‰§è¡Œä¼ å…¥çš„å‡½æ•°ï¼Œå°†Queueå’Œå…¶ä»–*argsã€**kwargsä½œä¸ºå‚æ•°ä¼ é€’
    p = Process(target=func, args=(q, *args), kwargs=kwargs)
    p.start()
    # ç­‰å¾…è¿›ç¨‹å®Œæˆæˆ–è¶…æ—¶
    p.join(timeout)
    if p.is_alive():
        print(f"{datetime.now()} [INFO] Function {str(func)} running timeout ({timeout}s), terminating...")
        p.terminate()  # ç»ˆæ­¢è¿›ç¨‹
        p.join()  # ç¡®ä¿è¿›ç¨‹å·²ç»ç»ˆæ­¢
        result = None  # è¶…æ—¶æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ²¡æœ‰ç»“æœ
    else:
        print(f"{datetime.now()} [INFO] Function {str(func)} executed successfully.")
        result = q.get()  # ä»é˜Ÿåˆ—ä¸­è·å–ç»“æœ
    return result

from difflib import unified_diff
from IPython.display import display, HTML

def generate_diff_html(text1, text2):
    diff = unified_diff(text1.splitlines(keepends=True),
                        text2.splitlines(keepends=True),
                        fromfile='text1', tofile='text2')

    diff_html = ""
    for line in diff:
        if line.startswith('+'):
            diff_html += f"<div style='color:green;'>{line.rstrip()}</div>"
        elif line.startswith('-'):
            diff_html += f"<div style='color:red;'>{line.rstrip()}</div>"
        elif line.startswith('@'):
            diff_html += f"<div style='color:blue;'>{line.rstrip()}</div>"
        else:
            diff_html += f"{line.rstrip()}<br>"
    return diff_html

newline_char = '\n'

# =============================================================================
# åœ¨åŸæœ‰ç¨‹å¼ç¢¼å¾Œæ·»åŠ æ¸…é™¤å‡½æ•¸
# =============================================================================

def clear_func():
    return "", "", ""

def clear_course_func():
    return None, "", "", ""

# =============================================================================
# Gradio ä»‹é¢
# =============================================================================

page_title = "RAT: AI Course Outline Generation System"
page_md = """
# RAT: AI Course Outline Generation System

With google search and file analysis, automatically generate professional course outlines.

Now model is: """ + get_available_model().upper() + """

---

## ğŸ“‹ System Features

### ğŸ¯ **General Q&A (RAT)**
- Intelligent draft generation
- Web information retrieval
- Multi-round answer optimization
- Structured responses

### ğŸ“š **Course Outline Generation**
- File analysis (PDF/DOCX/TXT)
- Web resource search
- Professional outline design
- Multi-stage optimization

---
"""

def process_course_generation(file, requirements, enable_search):
    """å¢å¼·ç‰ˆèª²ç¨‹å¤§ç¶±ç”Ÿæˆä¸»å‡½æ•¸"""
    if file is None and not requirements.strip():
        return "âš ï¸ Please upload a course document or enter course requirements.", ""
    
    # è™•ç†ä¸Šå‚³çš„æª”æ¡ˆï¼ˆæ”¯æ´å¤§æª”æ¡ˆï¼‰
    course_info = ""
    if file is not None:
        print(f"[INFO] è™•ç†ä¸Šå‚³æª”æ¡ˆ: {file.name}")
        file_content = process_uploaded_file(file)
        
        if file_content and "éŒ¯èª¤" not in file_content:
            # æª¢æŸ¥æª”æ¡ˆå¤§å°ï¼Œå¦‚æœå¤ªå¤§å‰‡åˆ†å¡Šè™•ç†
            file_tokens = num_tokens_from_string(file_content)
            print(f"[INFO] æª”æ¡ˆå…§å®¹ tokens: {file_tokens}")
            
            if file_tokens > 4000:
                print(f"[INFO] æª”æ¡ˆå…§å®¹éé•·ï¼Œé€²è¡Œåˆ†å¡Šæå–...")
                course_info = extract_course_info_chunked(file_content)
            else:
                course_info = extract_course_info(file_content)
        else:
            return f"âŒ File processing failed: {file_content}", ""

    # å¦‚æœæ²’æœ‰æª”æ¡ˆï¼Œä½¿ç”¨ç”¨æˆ¶éœ€æ±‚ä½œç‚ºèª²ç¨‹è³‡è¨Š
    if not course_info and requirements.strip():
        course_info = requirements
    
    # ä½¿ç”¨å¢å¼·çš„ RAT æŠ€è¡“ç”Ÿæˆèª²ç¨‹å¤§ç¶±
    try:
        print(f"[INFO] é–‹å§‹ä½¿ç”¨ RAT æŠ€è¡“ç”Ÿæˆèª²ç¨‹å¤§ç¶±...")
        base_outline, final_outline = generate_course_outline_with_rat(
            course_info, 
            requirements, 
            search_enabled=enable_search
        )
        return base_outline, final_outline
    except Exception as e:
        return f"âŒ Error occurred: {str(e)}", ""

def extract_course_info_chunked(file_content):
    """åˆ†å¡Šæå–èª²ç¨‹è³‡è¨Š"""
    chunks = chunk_texts(file_content, 2000)
    extracted_parts = []
    
    for i, chunk in enumerate(chunks):
        print(f"[INFO] æå–å¡Š {i+1}/{len(chunks)} çš„èª²ç¨‹è³‡è¨Š...")
        part_info = extract_course_info(chunk)
        extracted_parts.append(part_info)
    
    # åˆä½µæ‰€æœ‰æå–çš„è³‡è¨Š
    return combine_extracted_info(extracted_parts)

# =============================================================================
# Main Gradio Interface - English Version
# =============================================================================


with gr.Blocks(
    title=page_title,
    theme=gr.themes.Soft(),
    css="""
    /* å°å…¥å°ˆæ¥­å­—é«” */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Roboto:wght@300;400;500;700&display=swap');
    
    /* å…¨åŸŸå­—é«”è¨­å®š - ä½¿ç”¨æ›´éŠ³åˆ©çš„å•†å‹™å­—é«” */
    * {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* Gradio å®¹å™¨æ•´é«”å­—é«” */
    .gradio-container {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* æ¨™é¡Œå­—é«” - ä½¿ç”¨æ›´éŠ³åˆ©çš„å­—é«” */
    h1, h2, h3, h4, h5, h6 {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
        font-weight: 600 !important;
        letter-spacing: -0.01em !important;
    }
    
    /* æŒ‰éˆ•å­—é«” */
    button {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
        font-weight: 500 !important;
        letter-spacing: 0 !important;
    }
    
    /* è¼¸å…¥æ¡†å’Œæ–‡å­—æ¡†å­—é«” */
    input, textarea {
        font-family: 'Roboto', 'Consolas', 'Monaco', 'Courier New', monospace !important;
        font-size: 14px !important;
        line-height: 1.5 !important;
    }
    
    /* æ¨™ç±¤å­—é«” */
    label {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
        font-weight: 500 !important;
    }
    
    /* Markdown å…§å®¹å­—é«” */
    .markdown {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* ç¨‹å¼ç¢¼å­—é«” */
    code, pre {
        font-family: 'Consolas', 'Monaco', 'Courier New', 'Roboto Mono', monospace !important;
        font-size: 13px !important;
    }
    
    /* ä¸»è¦æ¨™é¡Œå€åŸŸ */
    .main-header { 
        text-align: center; 
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    .main-header h1 {
        font-weight: 700 !important;
        letter-spacing: -0.02em !important;
    }
    
    /* åŠŸèƒ½å€å¡Š */
    .feature-box {
        border: 1px solid #e1e5eb;
        border-radius: 8px;
        padding: 15px;
        margin: 10px 0;
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* ç‹€æ…‹é¡è‰² */
    .status-success { 
        color: #28a745; 
        font-weight: bold;
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    .status-error { 
        color: #dc3545; 
        font-weight: bold;
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    .status-warning { 
        color: #ffc107; 
        font-weight: bold;
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* Tab æ¨™ç±¤å­—é«” */
    .tab-nav button {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
        font-weight: 500 !important;
    }
    
    /* å´é‚Šæ¬„å­—é«” */
    .gr-box {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* ç¯„ä¾‹å€åŸŸå­—é«” */
    .gr-examples {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* è³‡è¨Šæ¡†å­—é«” */
    .gr-info {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    
    /* é è…³å­—é«” */
    .footer {
        font-family: 'Inter', 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif !important;
    }
    """
) as demo:
    
    gr.HTML(f"""
    <div class="main-header">
        <h1>ğŸ§  RAT: Retrieval-Augmented Thoughts</h1>
        <p>Intelligent Retrieval-Augmented Thinking System | Current Model: <strong>{get_available_model().upper()}</strong></p>
    </div>
    """)
    
    with gr.Tabs():
        
        # =============================================================================
        # Tab 1: Course Outline Generation
        # =============================================================================
        with gr.Tab("ğŸ“š Course Outline Generation", elem_id="course-tab"):
            gr.Markdown("""
            ### ğŸ¯ Feature Description
            Upload course-related documents or input requirements, the system will automatically search web resources and generate professional course outlines
            """)
            
            with gr.Row():
                with gr.Column(scale=1, elem_classes="feature-box"):
                    gr.Markdown("#### ğŸ“ Input Settings")
                    
                    file_upload = gr.File(
                        label="ğŸ“ Upload Course Document",
                        file_types=[".pdf", ".docx", ".txt"],
                        type="filepath",
                        height=100
                    )
                    
                    requirements_box = gr.Textbox(
                        label="ğŸ“ Course Requirements Description",
                        placeholder="Please describe your course objectives, audience, duration, key content and other requirements in detail...\n\nExample:\nâ€¢ Python Programming Fundamentals Course\nâ€¢ For first-year university students\nâ€¢ 16-week course including basic syntax, data structures, project implementation",
                        lines=6,
                        max_lines=10
                    )
                    
                    with gr.Row():
                        enable_search = gr.Checkbox(
                            label="ğŸŒ Enable Web Search Enhancement",
                            value=True,
                            info="Search relevant teaching resources to enrich course content"
                        )
                        
                    with gr.Row():
                        generate_course_btn = gr.Button(
                            "ğŸš€ Generate Course Outline",
                            variant="primary",
                            size="lg"
                        )
                        clear_course_btn = gr.Button(
                            "ğŸ—‘ï¸ Clear",
                            variant="secondary"
                        )
                
                with gr.Column(scale=2):
                    with gr.Tabs():
                        with gr.Tab("ğŸ“‹ Basic Outline"):
                            gr.Markdown("#### ğŸ“‹ Basic Course Outline")
                            base_outline_box = gr.Textbox(
                                placeholder="Basic weekly course outline will be displayed here...",
                                lines=20,
                                max_lines=25,
                                interactive=True,  # å…è¨±ç”¨æˆ¶é¸å–æ–‡å­—
                                show_copy_button=False, 
                            )
                        
                        with gr.Tab("âœ¨ Enhanced Outline"):
                            gr.Markdown("#### âœ¨ Enhanced Course Outline")
                            final_outline_box = gr.Textbox(
                                placeholder="Enhanced weekly course outline with web resources will be displayed here...",
                                lines=20,
                                max_lines=25,
                                interactive=True,
                                show_copy_button=False,
                            )
            
            # Course Examples
            with gr.Accordion("ğŸ’¡ Usage Examples", open=False):
                course_examples = gr.Examples(
                    examples=[
                        ["Python Programming Fundamentals Course for first-year university students, 16-week course including basic syntax, data structures, algorithms, GUI programming and project implementation"],
                        ["Machine Learning Fundamentals Course for professionals with programming background, 12-week online course covering supervised learning, unsupervised learning and deep learning"],
                        ["Digital Marketing Strategy Course for business executives, 8-week practical course including SEO, social media, content marketing and data analytics"],
                        ["Data Science Project Implementation, graduate level, combining theory and practice, including data collection, cleaning, analysis, visualization and model construction"],
                        ["Full-Stack Web Development Course, 16-week course from HTML/CSS basics to React frontend and Node.js backend development"],
                    ],
                    inputs=[requirements_box],
                    label="Click examples to get started quickly"
                )
            
            generate_course_btn.click(
                fn=process_course_generation,
                inputs=[file_upload, requirements_box, enable_search],
                outputs=[base_outline_box, final_outline_box],
                show_progress=True
            )
            
            clear_course_btn.click(
                fn=clear_course_func,
                outputs=[file_upload, requirements_box, base_outline_box, final_outline_box]
            )
        
        # =============================================================================
        # Tab 2: System Information
        # =============================================================================
        with gr.Tab("âš™ï¸ System Information", elem_id="info-tab"):
            gr.Markdown("""
            ### ğŸ“Š System Status
            """)
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown(f"""
                    #### ğŸ¤– Model Configuration
                    - **Current Model**: {get_available_model().upper()}
                    - **OpenAI API**: {'âœ… Configured' if os.getenv('OPENAI_API_KEY') else 'âŒ Not Configured'}
                    - **Google Search API**: {'âœ… Configured' if os.getenv('GOOGLE_API_KEY') else 'âŒ Not Configured'}
                    
                    #### ğŸ” RAT Processing Flow
                    1. **Draft Generation**: Generate initial response based on the question
                    2. **Smart Segmentation**: Break down answers into logical paragraphs
                    3. **Query Generation**: Generate search queries for each segment
                    4. **Web Retrieval**: Search relevant information for verification
                    5. **Answer Optimization**: Integrate retrieved information to improve answers
                    6. **Final Formatting**: Present final results in structured format
                    """)
                
                with gr.Column():
                    gr.Markdown("""
                    #### ğŸ“š Course Outline Generation Flow
                    1. **File Analysis**: Extract key information from uploaded documents
                    2. **Requirements Understanding**: Analyze user's course requirements
                    3. **Basic Outline**: Generate initial course structure
                    4. **Resource Search**: Find relevant teaching resources
                    5. **Content Enhancement**: Optimize outline by integrating search results
                    6. **Professional Formatting**: Generate standardized course outline
                    
                    #### ğŸ“ Supported File Formats
                    - **PDF** (.pdf) - Academic papers, course syllabi
                    - **Word** (.docx) - Course plans, teaching documents
                    - **Text Files** (.txt) - Plain text course materials
                    """)
            
            with gr.Accordion("ğŸ”§ Advanced Settings", open=False):
                gr.Markdown("""
                #### Environment Variables Configuration
                ```bash
                # Model Settings
                export MODEL_TYPE=openai
                export OPENAI_API_KEY="your-openai-key"
                
                # Google Search Settings
                export GOOGLE_API_KEY="your-google-key"
                export GOOGLE_CSE_ID="your-cse-id"
                
                # System Parameter Adjustments
                export RAT_TIMEOUT=30
                export RAT_CONTENT_LIMIT=2
                ```
                
                #### Performance Optimization Recommendations
                - **Memory**: Recommended 8GB+ (if using Ollama)
                - **Network**: Stable internet connection for API calls
                - **Concurrency**: System supports timeout handling and error recovery
                """)

    # =============================================================================
    # é è…³è³‡è¨Š
    # =============================================================================
    gr.HTML("""
    <div style="text-align: center; margin-top: 30px; padding: 20px; background-color: #f8f9fa; border-radius: 10px;">
        <p><strong>ğŸ§  RAT: Retrieval-Augmented Thoughts System</strong></p>
        <p>Intelligent Retrieval-Augmented Thinking System - Powerful tool combining AI reasoning with web search</p>
        <p><small>Version 2.0 | Supports Course Outline Generation & Intelligent Q&A</small></p>
    </div>
    """)

# =============================================================================
# å•Ÿå‹•æ‡‰ç”¨
# =============================================================================

if __name__ == "__main__":
    print(f"{datetime.now()} [INFO] Checking model config...")
    check_model_config()
    print(f"{datetime.now()} [INFO] Checking Google Search API key...")
    check_search_config()
    print(f"{datetime.now()} [INFO] Starting Gradio app...")
    # å•Ÿå‹• Gradio æ‡‰ç”¨
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )
